# -*- coding: utf-8 -*-
"""dev_nlp

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YA93KL7q2NXS8cCI5StZhzZUs81xFj0h
"""

# ============================================
# ðŸ“Œ NLP Phases for Fake vs Real Detection
# Using Naive Bayes at each step - REVISED FOR COLAB
# ============================================

# Install dependencies (Colab often has scikit-learn, pandas, nltk, spacy, textblob pre-installed,
# but it's good practice to ensure the latest or required versions are there).
# Note: 'pip install scikit-learn pandas nltk spacy textblob' is not needed if the base Colab environment is used.
# Let's keep the spacy download command as it ensures the model is present.

!pip install scikit-learn pandas nltk spacy textblob # Generally pre-installed, but for completeness
!python -m spacy download en_core_web_sm # Ensure spacy model is downloaded

# ============================
# Imports and Setup
# ============================
import pandas as pd
import numpy as np
import nltk, re, string, spacy
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Download resources
# Colab uses a shared environment, so downloads are usually needed only once per session.
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
# nltk.download('punkt_tab') # This is likely an error/typo, removing the redundant download

# load spaCy English model
# Ensure this runs after the spacy download command
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    print("SpaCy model 'en_core_web_sm' not found. Please run the download command and try again.")
    # Exit or raise error if crucial
    raise

# ============================
# Step 1: Load Dataset (Colab Specific Change)
# ============================
# The original path "C:/Users/..." is a local Windows path and won't work in Colab.
# We need to upload the file and then read it, or load it from a mounted Google Drive.

# --- INSTRUCTIONS FOR COLAB USAGE ---
# 1. You MUST upload 'merged_final.csv' to your Colab session's files (left sidebar -> Files icon).
# 2. After uploading, the file will be in the current working directory, which is '/content/'.
# --------------------------------------

# NOTE: If you run this block, Colab will prompt you to upload the file if it's not present.
from google.colab import files
import os

file_name = "merged_final.csv"

# Check if the file is already uploaded
if not os.path.exists(file_name):
    print("Uploading file... Please select 'merged_final.csv'")
    uploaded = files.upload()
    if file_name not in uploaded:
        print(f"Error: '{file_name}' not uploaded. Please upload the correct file.")
        raise FileNotFoundError(f"'{file_name}' not found in uploaded files.")

df = pd.read_csv(file_name) # Use the correct file name assuming it's uploaded

print("Dataset Shape:", df.shape)
print("Columns:", df.columns.tolist())
print(df.head())

X = df['statement']
y = df['BinaryTarget']

# ============================
# Helper: Train NB Model
# ============================
def train_nb(X_features, y, name):
    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)
    model = MultinomialNB()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"\nðŸ”¹ {name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))
    return acc

# ====================================================================================================
# Phase 1: Lexical & Morphological Analysis
# (Tokenization + Stopword removal + Lemmatization)
# ====================================================================================================
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def lexical_preprocess(text):
    if pd.isna(text): # Handle potential NaN values
        return ""
    text = str(text).lower() # Ensure it's a string and lowercase
    tokens = nltk.word_tokenize(text)
    # Remove punctuation during the filtering process for cleaner tokens
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and w not in string.punctuation]
    return " ".join(tokens)

X_lexical = X.apply(lexical_preprocess)
vec_lexical = CountVectorizer().fit_transform(X_lexical)
acc1 = train_nb(vec_lexical, y, "Lexical & Morphological Analysis")

print("-" * 50)

# ====================================================================================================
# Phase 2: Syntactic Analysis (Parsing)
# (Extract POS tags & dependency relations as features)
# ====================================================================================================
def syntactic_features(text):
    if pd.isna(text):
        return ""
    doc = nlp(str(text))
    # Using POS tags as features
    pos_tags = " ".join([token.pos_ for token in doc])
    return pos_tags

X_syntax = X.apply(syntactic_features)
# Use the default vectorizer for POS tags
vec_syntax = CountVectorizer().fit_transform(X_syntax)
acc2 = train_nb(vec_syntax, y, "Syntactic Analysis")

print("-" * 50)

# ====================================================================================================
# Phase 3: Semantic Analysis
# (Polarity & Subjectivity from TextBlob)
# ====================================================================================================
def semantic_features(text):
    if pd.isna(text):
        # Return neutral sentiment if text is missing
        return f"0.0 0.0"
    blob = TextBlob(str(text))
    # Features are returned as two space-separated string values: polarity and subjectivity
    return f"{blob.sentiment.polarity} {blob.sentiment.subjectivity}"

X_semantic = X.apply(semantic_features)

# Since the features are numerical (polarity and subjectivity) represented as strings,
# TfidfVectorizer or CountVectorizer will treat them as tokens "0.5" or "0.8", which is not ideal.
# Ideally, these would be separate numerical columns, but to fit the existing 'vectorizer' structure
# and the NB model (which expects counts/TF-IDF), we'll keep the TfidfVectorizer as in the original.
vec_semantic = TfidfVectorizer().fit_transform(X_semantic)
acc3 = train_nb(vec_semantic, y, "Semantic Analysis")

print("-" * 50)

# ====================================================================================================
# Phase 4: Discourse Integration
# (Sentence relations, connectives, length features) - Simplified for this example
# ====================================================================================================
def discourse_features(text):
    if pd.isna(text):
        return "0" # No sentences

    text = str(text)
    sentences = nltk.sent_tokenize(text)
    num_sentences = len(sentences)

    # Feature 2: First word of each sentence (a proxy for sentence relations/connectives)
    first_words = [s.split()[0].lower() for s in sentences if len(s.split()) > 0 and s.split()[0].lower() not in string.punctuation]

    # Return space-separated feature tokens (sentence count and first words)
    return f"{num_sentences} {' '.join(first_words)}"

X_discourse = X.apply(discourse_features)
vec_discourse = CountVectorizer().fit_transform(X_discourse)
acc4 = train_nb(vec_discourse, y, "Discourse Integration")

print("-" * 50)

# ====================================================================================================
# Phase 5: Pragmatic Analysis
# (Contextual features: interrogative, exclamatory, modality words)
# ====================================================================================================

pragmatic_words = ["must", "should", "might", "could", "will", "?", "!"]

def pragmatic_features(text):
    features = []
    if pd.isna(text):
        return ""

    text_lower = str(text).lower()
    for w in pragmatic_words:
        count = text_lower.count(w)
        # Add feature token only if count > 0
        if count > 0:
            features.extend([w] * count) # repeat word count times
    # Return space-joined tokens (words), not counts
    return " ".join(features)


X_pragmatic = X.fillna('').astype(str).apply(pragmatic_features)
# Filter out rows where no pragmatic words/punctuation were found, as the vectorizer will be empty for them.
non_empty_idx = X_pragmatic.str.strip() != ''
X_pragmatic_filtered = X_pragmatic[non_empty_idx]
y_filtered = y[non_empty_idx] # Filter target accordingly!

# Initialize acc5 to 0.0 in case the dataset is filtered to empty
acc5 = 0.0

if X_pragmatic_filtered.empty:
    print("No meaningful pragmatic features found for vectorization.")
else:
    # Use CountVectorizer to count the occurrences of the pragmatic words/symbols
    vec_pragmatic = CountVectorizer(stop_words=None).fit_transform(X_pragmatic_filtered)
    acc5 = train_nb(vec_pragmatic, y_filtered, "Pragmatic Analysis")

print("-" * 50)

# ============================
# Final Results
# ============================
print("\nðŸ“Š Phase-wise Naive Bayes Accuracies:")
print(f"1. Lexical & Morphological: {acc1:.4f}")
print(f"2. Syntactic: {acc2:.4f}")
print(f"3. Semantic: {acc3:.4f}")
print(f"4. Discourse: {acc4:.4f}")
print(f"5. Pragmatic: {acc5:.4f}")

